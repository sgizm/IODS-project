## Clustering and Cassification Exercise
17.02.2017

We start with reading the Boston file right after including the libraries we need as usual:

```{r include = FALSE}
library(dplyr)
library(GGally)
library(ggplot2)
library(boot)
library(corrplot)
library(MASS)
```
```{r}
# loading the data
data("Boston")

# exploring
str(Boston)
dim(Boston)
summary(Boston)
```

When we look at the variables we see housing values in the suburbs of Boston. There seem to be 14 variables and 506 observations.  All of the variables are continuous. The variable crim, that we will be mostly dealing with, refers to per capita crime rate. Graphical view:

```{r}
pairs(Boston)
```

We can see that some variables are actually strongly correlated, but perhaps it will help more if we look at the numbers with the correlations:

```{r}
cor_matrix <- cor(Boston) %>% round(digits = 2)
cor_matrix
```

Still, it will help to visualize the correlations:
```{r}
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex=0.6)
```

Now the correlations can be interpreted better. Since we will be mostly working with the variable crim, let's look at that. We see that crim is correlated with the variables rad (cor=0.63) and tax (cor=0.58). Rad was showing index of accessibility to radial highways
whereas tax indicates full-value property-tax rate per $10,000. Overall, strongest correlation seems to be between rad and tax variables. On the other end, we also see negative strong correlations such as indus/dis, nox/dis and lstat/medv.

#### Standardization
We need to standardize the variables so that they fit the assumptions held by the classification method and the compare the variables to each other. Classification method assumes that the variables are normally distributed and variance is the same. 
```{r}
# standardizing the variables and the summary
boston_scaled <- scale(Boston)
summary(boston_scaled)
```
Next will be working on the crim variable:

```{r}
# putting it in a df
boston_scaled <- as.data.frame(boston_scaled)
scaled_crim <- boston_scaled$crim
# creating a quantile vector of crim
bins <- quantile(scaled_crim)
# creating the categorical variable 'crime' and summarizing 
crime <- cut(scaled_crim, breaks = bins, include.lowest = TRUE, labels=c("low", "med_low", "med_high", "high"))
summary(crime)
# dropping the old crime rate variables as asked
boston_scaled <- dplyr::select(boston_scaled, -crim)
# and adding the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
```
What is next is that the dataset needs to be divided into a training set and a testing set. nrow() will be used to get the number of rows and  select 80% of it randomly:

```{r}
n <- nrow(boston_scaled)
ind <- sample(n,  size = n * 0.8)
# further creating the train and test sets
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
```

#### LDA

LDA is a classification method where the classes are known upfront. We will be using our already created classes:

```{r}
lda.fit <- lda(crime ~ ., data = train)
lda.fit

# the function for lda biplot arrows gotten from datacamp
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

classes <- as.numeric(train$crime)
# plot the lda results
plot(lda.fit, col= classes, pch = classes,  dimen = 2)
lda.arrows(lda.fit, myscale = 1)
```

From the plot we can see that rad variable, that is radial highways, is associated with high crime rate. Next we will save the correct classes from the test data and remove the crime variable. After that, we will predict the classes with test data.
```{r}
# saving the correct classes from test data
correct_classes <- test$crime
# removing the crime variable from test data
test <- dplyr::select(test, -crime)
# predict with test data
lda.pred <- predict(lda.fit, newdata = test)
# cross tabulating the results
table(correct = correct_classes, predicted = lda.pred$class)
```

Results show that high crime rates were predicted quite good, whereas low ones show some uncertanities. 

#### Distance and clusters

After reloading and scaling the data we will look at the distance measures.

```{r include= FALSE}
# reloading the data, scaling and dataframing
data("Boston")
boston_scaled <- scale(Boston)
boston_scaled <- as.data.frame(boston_scaled)
# euclidean distance matrix
dist_eu <- dist(Boston)
# the summary of the distances
summary(dist_eu)
# manhattan distance matrix
dist_man <- dist(Boston, method = 'manhattan')
# the summary of the distances
summary(dist_man)
```

Then we run the k-means clustering function:
```{r}
km <-kmeans(dist_eu, centers = 15)
# and ploting the Boston dataset with clusters
pairs(Boston, col = km$cluster)
```

Next is trying to find out the right number of clusters. We start with 10 as done in the datacamp. Then, we will use WCSS, that calculates the number of clusters for which the observations are closest to the center of the cluster. 

```{r}
set.seed(123)
k_max <- 10
# calculating the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(dist_eu, k)$tot.withinss})
# visualizing the results
plot(1:k_max, twcss, type='b')
```
We see that where k_max = 2, there is a significant drop in the line graph, indicating that optimum number of classes is 2. Therefore, we k-means with centers = 2:

```{r}
# k-means clustering
km <-kmeans(dist_eu, centers = 2)
# ploting the Boston dataset with clusters
pairs(Boston, col = km$cluster)
```

Two colors indicate the different classes, however, it is not easy to distinguish the correlations overall. Yet, crime variable is the most distinguisable with other variables.